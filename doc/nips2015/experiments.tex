\section{Experiments}
\label{sec:experiments}

We run our active classifier on the above policies, and compare against machine-only and human-only baselines, as well as a baseline policy that uses a threshold-version of uncertainty sampling without any effort to minimize loss.
We report results on several datasets: CoNLL NER (restricted to PER and LOC tags), Stanford sentiment classification, and a facial identification task.

Since we observed high variance on the quality of crowd workers, we ``freeze'' the crowd by asking for a batch of labels offline, and evaluating by drawing from the human opinions (with accompanying observed delay) when the algorithm wants a human observation.
This controls for worker quality, by ensuring that all the compared approaches receive the same results when querying for human labels.

We also run several ``live'' experiments with the crowd, though these are not comparable with the frozen runs, because we cannot control for variance in worker quality.

\subsection{CoNLL 2-Class Evaluation}

The CoNLL NER task asks an algorithm to distinguish between 5 possible tags for every token: \{\textbf{PER},\textbf{LOC},\textbf{ORG},\textbf{MISC},\textbf{O}\}.
We found in experiments that Turkers (and ourselves) had trouble with \textbf{MISC}, and often confused \textbf{ORG} and \textbf{LOC}, even in the presence of a detailed tutorial.

In order to minimize this confusion, we use a restricted dataset, of only \{\textbf{PER}, \textbf{LOC}, \textbf{O}\}.
Our CoNLL NER dataset is composed of 1040 sentences, drawn from the CoNLL NER training set, downloadable from \todo{link}.

The classifier is given 40 examples `gratis.' It is then run on the remaining 1000 sentences, drawing samples from the frozen pool of crowd responses when required, and performance is reported.

\begin{tabular}{ | l | r | r | r | r | r | }
    \hline
    System & Avg classification time/token & Avg requests/token & Precision & Recall & F1 \\ \hline
    Human 1-query baseline & 664 ms & 1.0 & 66.38 & 89.58 & 76.15 \\ \hline
    Human 3-query baseline & 1495 ms & 3.0 & 92.79 & 89.56 & 91.58 \\ \hline
    Offline baseline & n/a & n/a & 62.38 & 69.76 & 65.86 \\ \hline
    Threshold baseline & 1523 ms & 0.65 & 91.74 & 90.90 & 91.33 \\ \hline
    Time insensitive MCTS & 3368 ms & \textbf{0.62} & 94.32 & 93.16 & \textbf{93.73} \\ \hline
    \hline
\end{tabular}

To validate that this algorithm is in fact performing as we expect, we plot several graphs over the course of our run. \todo{insert}

\subsection{Movie-review Sentiment Evaluation}

The Stanford Sentiment dataset consists of several thousand highly-polar movie reviews.

\subsection{Facial Identification Evaluation}

The Facial Identification dataset is a constructed subset of \todo{Princeton something-or-other}.

\todo{write}

