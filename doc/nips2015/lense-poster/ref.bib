@inproceedings{Simsek,
author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Barto, Andrew G},
booktitle = {NIPS},
file = {:home/teju/mendeley-papers/Şimşek, Barto/NIPS/Şimşek, Barto - 2008 - Skill characterization based on betweenness.pdf:pdf},
pages = {1--8},
title = {{Skill characterization based on betweenness}},
year = {2008}
}

@inproceedings{Martel2004,
author = {Martel, Chip and Nguyen, Van},
booktitle = {PODC},
file = {:home/teju/mendeley-papers/Martel, Nguyen/PODC/Martel, Nguyen - 2004 - Analyzing Kleinberg ’s (and other) Small-world Models.pdf:pdf},
isbn = {1581138024},
keywords = {diameter,random graphs,routing,small-world network},
title = {{Analyzing Kleinberg's (and other) Small-world Models}},
volume = {2},
year = {2004}
}

@article{Kleinberg,
author = {Kleinberg, Jon},
file = {:home/teju/mendeley-papers/Kleinberg/Unknown/Kleinberg - Unknown - The Small-World Phenomenon An Algorithmic Perspective.pdf:pdf},
pages = {1--14},
title = {{The Small-World Phenomenon : An Algorithmic Perspective}}
}

@article{Stolle,
abstract = {Temporally extended actions (e.g., macro actions) have proven very useful in speeding up learning, ensuring robustness and building prior knowledge into AI systems. The options framework (Precup, 2000; Sutton, Precup \& Singh, 1999) provides a natural way of incorporating such actions into reinforcement learning systems, but leaves open the issue of how good options might be identi- fied. In this paper, we empirically explore a simple approach to creating options. The underlying assumption is that the agent will be asked to perform different goal-achievement tasks in an environment that is otherwise the same over time. Our approach is based on the intuition that “bottleneck” states, i.e. states that are frequently visited on system trajectories, could prove to be useful subgoals (e.g. McGovern \& Barto, 2001; Iba, 1989). We present empirical studies of this approach in two gridworld navigation tasks. One of the environments we explored contains bottleneck states, and the algo- rithm indeed finds these states, as expected. The second environment is an empty gridworld with no obstacles. Although the environment does not contain bottle- neck states, our approach still finds useful options, which essentially allow the agent to travel around the environment more quickly},
author = {Stolle, Martin and Precup, Doina},
file = {:home/teju/mendeley-papers/Stolle, Precup/Artificial Intelligence/Stolle, Precup - Unknown - Learning Options in Reinforcement Learning.pdf:pdf},
journal = {Artificial Intelligence},
title = {{Learning Options in Reinforcement Learning}}
}

@article{SuttonPrecupSingh1998,
author = {Sutton, Richard S and Precup, Doina and Singh, Satinder},
file = {:home/teju/mendeley-papers/Sutton, Precup, Singh/Artificial Intelligence/Sutton, Precup, Singh - 1998 - Between MDPs and Semi-MDPs Learning , Planning , and Representing Knowledge at Multiple Temporal Scales at Multiple Temporal Scales.pdf:pdf},
journal = {Artificial Intelligence},
title = {{Between MDPs and Semi-MDPs : Learning , Planning , and Representing Knowledge at Multiple Temporal Scales at Multiple Temporal Scales}},
year = {1998}
}

@article{Simsek2005,
address = {New York, New York, USA},
author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Wolfe, Alicia P. and Barto, Andrew G.},
doi = {10.1145/1102351.1102454},
file = {:home/teju/mendeley-papers/Şimşek, Wolfe, Barto/Proceedings of the 22nd international conference on Machine learning - ICML '05/Şimşek, Wolfe, Barto - 2005 - Identifying useful subgoals in reinforcement learning by local graph partitioning.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
pages = {816--823},
publisher = {ACM Press},
title = {{Identifying useful subgoals in reinforcement learning by local graph partitioning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102454},
year = {2005}
}

@inproceedings{Menache,
author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
booktitle = {ECML},
file = {:home/teju/mendeley-papers/Menache, Mannor, Shimkin/ECML/Menache, Mannor, Shimkin - 2002 - Q-Cut - Dynamic Discovery of Sub-Goals in Reinforcement Learning.pdf:pdf},
title = {{Q-Cut - Dynamic Discovery of Sub-Goals in Reinforcement Learning}},
year = {2002}
}
