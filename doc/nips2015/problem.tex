\section{Setup}
\label{sec:problem}

We will now formalize how our task is evaluated, modelled and executed. 
We consider the ``designer'' to be the person who implements our system: she must decide what the model is and define an appropriate cost function.
A ``user'' of the system is the person providing input that needs to be labeled. 
Finally, ``labelers'' in the system provide potentially noisy partial labels (or measurements) on inputs from the user. \todo{diagram}
% Note that the designer, user and labeler can all be the same person.

\subsection{Measuring progress.}

First, let's look at how answers produced by the system are evaluated.
In the conventional supervised setting, a natural metric is the accuracy of our predictions relative to the true labels in the training data.
The system gets to see these true labels and is able to optimize an appropriate loss.

Formally, let $\bx\oft{1}, \dots, \bx\oft{t}, \dots, \bx\oft{T}$ be a sequence of inputs, 
let $(\by\oft{t})$ be their true labels,
and let $(\byt\oft{t})$ be the sequence of predictions made by the system.
We measure the performance of our system using the loss $\ell(\by\oft{t}, \byt\oft{t})$, and our objective is to minimize the loss on the dataset: $\sum_{t=1}^T \ell(\by\oft{t}, \byt\oft{t})$.

When constructing datasets from user interactions, the system does not get to observe $\by\oft{t}$ or $\ell(\by\oft{t}, \byt\oft{t})$.
We are only aware of the true labels if and when we query a human annotator: simply measuring performance incurs a cost for our system.
Let $\sigma\oft{t}$ be the measurements queried by the system, incurring a cost $C(\sigma\oft{t})$.
Our goal is to minimize the objective
\begin{align*}
  \sL &= \sum_{t=1}^T \ell(\by\oft{t}, \byt\oft{t}) + C(\sigma\oft{t}).
\end{align*}

\subsection{Model}

We consider the family of conditional exponential models:
\begin{align*}
  p_\theta(y \given x) 
  &= \exp( \theta^\top \phi(x, y) - A(\theta; x)),
\end{align*}
where $A(\theta; x)$ is the conditional log-normalizer.
We assume the model has low treewidth or otherwise admits efficient marginal computation.

As mentioned earlier, we do not have any labeled examples $y$, but can ask for some measurement $\sigma \in \Sigma$ by asking the crowd. 
In order to incorporate this information, we assume the distribution of responses to be modeled as an exponential measurement model as in~\cite{liang09measurements}:
\begin{align*}
  p(\tau \given x, y, \sigma) 
  &\propto \exp \left( h_\sigma(\tau - \sigma(x,y)) \right),
\end{align*}
where $h_\sigma$ is a convex function \todo{(arun): This is currently stupid. Motivate $h_\sigma$}.
Consider a simple model for noisy labeling, where a person returns the true label with a uniform probability of $1- \epsilon$, and guesses at random otherwise. In this case, 
\begin{align*}
  h_\sigma(u) &= \alpha \I(\tau \neq \sigma),
\end{align*}
where $\alpha$ is equal to the inverse sigmoid:$\alpha = \sigma\inv(1 - \epsilon/2)$.

Typical examples of measurements are partial labels.
We assume that incorporating these measurements can be done efficiently, i.e.\ marginals can be efficiently computed for the joint model,
\begin{align*}
  p_\theta(y \given x, \tau, \sigma) 
  &\propto \exp(\theta^\top \phi(x, y) + h_\sigma(\tau - \sigma(x,y))).
\end{align*}

\section{Measurement Selection}
\label{sec:measurement}

The key computation we need to perform is to choose a measurement action.
Assume that $\Sigma = \{\sigma_0, \sigma_1, \dots, \sigma_n\}$ are the set of valid measurement operations one can perform. 
Furthermore, let $\sigma_0$ be a special operation that does not ask for any measurement: it returns the most probable  labeling.

\subsection{Simple model}
\label{sec:simple}

Let $\ell(\by, \byt)$ be the loss incurred when if $\by$ is labeled $\byt$.
When presented with an example $\bx$ to label, our system estimates a loss of $\E_{p(\by \given \bx)}[\ell(\by, \byt)]$, where $\byt = \argmax_{\by} \p(\by \given \bx)$.
If we performed the measurement operator $\sigma$ and received a measurement $\tau$,
then our expected loss would be $\E_{p(\by \given \bx, \tau)}[\ell(\by, \byt(\tau))]$, where $\byt(\tau) = \argmax_{\by} p(\by \given \bx, \tau)$.
Intuitively, if we had perfect feedback, observing $\tau$ would provide use more information, reducing our risk.
However, taking measurements has an associated cost, $C(\sigma)$, a function of time and money that the designer must choose.
There is also a possibility that the measurement does not return a value (because of a timeout).

Let the CDF be $F_\sigma(t)$.
We model the utility of a particular measurement operation, given a time window $t_0$ to be:
\begin{align*}
U(\sigma)
&= F_\sigma(t_0) 
  \E_{p(\tau \given x, \sigma)} \left[\E_{p(\by \given \bx, \tau)}[\ell(\by, \byt(\tau))] \right]
  + (1 - F_\sigma(t_0)) 
    \left[\E_{p(\by \given \bx)}[\ell(\by, \byt)] \right]
  + C(\sigma).
\end{align*}

Without loss of generality, assume that the null measurement is free: $C(\sigma_0) = 0$.
Intuitively, this ensures that we will only ever choose to measure something if the expected reduction in risk is more than the cost of executing the measurement.

Let the label $\by$ have $n$ components: $\by = (y_1, ..., y_n)$.
Further, let us assume that the loss function $\ell$ decomposes over labels: $\ell(\by, \byt) = \sum_{i=1}^n \ell(y_i, \yt_i)$. 
Under this assumption, the expected utility of a single measurement operator $\sigma$ can be efficiently computed with $2L$ inference calls\footnote{The marginal inference query in lines 6 and 7 of \algorithmref{expected-utility} can be shared.} to the model using \algorithmref{expected-utility}.

The measurement operator to take is simply $\sigma^* = \argmin_{\sigma \in \Sigma} U(\sigma)$.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
  \caption{Computing expected utility $U(\sigma)$}
  \label{algo:expected-utility}
  \begin{algorithmic}[1]
    \REQUIRE Measurement operator $\sigma$, input $\bx$, models $p_\theta(\by \given \bx)$ and $p_\theta(\by \given \bx, \tau, \sigma)$, $F_\sigma$ and $t_0$.
    \ENSURE Expected utility $U(\sigma)$
    \STATE Let $y_\sigma$ be label(s) measured by operator $\sigma$.
    \STATE Compute $p_\theta(y_\sigma \given \bx)$ using marginal inference.
    \STATE Set $p_\theta(\tau \given \bx) \gets p_\theta(\tau \given y_\sigma, \bx) p_\theta(y_\sigma \given \bx)$.
    \STATE Initialize $u \gets (0, \dots, 0)$.
    \FORALL{$i \in [L]$}
    \STATE Compute $\byt = \argmax_{\by} p_\theta(\by \given \bx, \tau = i, \sigma)$ using marginal inference.
    \STATE Compute $p(y_j) = p_\theta(\by_j \given \bx, \tau = i, \sigma)$ for $j \in [n]$ using marginal inference.
    \STATE Update $u[i] \gets \p(\tau = i \given \bx) \E_{p(y_j)}[\ell(y_j, \yt_j)]$.
    \ENDFOR
    \STATE Return the expected utility: $\frac{\sum_{i=1}^L u[i] p_\theta(\tau = i \given x)}{\sum_{i=1}^L p_\theta(\tau = i \given x)}$
  \end{algorithmic}
\end{algorithm}

\subsection{Multiple asynchronous requests}
\label{sec:async}

For the system to be real-time, we need to dispatch multiple measurement queries at the same time.
Let $\sigma_1, \sigma_2, \dots, \sigma_n$ be the set of queries we can dispatch.

\begin{note}[Baseline: Next best policy]
\noteb{(arun): We should probably move this to experiments as a baseline or ignore all together.}
In this scheme we do not reason about the future and choose subsequent measurement operators by going down the ordered list of measurement utilities.
This approach, while simple, does not allow us to query the same node multiple times, which is often optimal if there is high uncertainty on a single important node.
\end{note}

We need to reason about the possible responses that might be returned.
For the sake of simplicity, we will choose (sequentially) the best set of $n$ measurements to make at the very beginning of our time window, not taking into account responses.

\algorithmref{expected-utility} can be trivially updated by incorporating previous measurement operators, say $\tau_1, \dots, \tau_{n-1}$.
Naively, this would require us to enumerate over $L^d$ possible values of $\tau_1, \dots, \tau_n$ in line 5 of \algorithmref{expected-utility}.
Instead, we propose using a particle filter to estimate utilities (\algorithmref{filtered-utility}).

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Computing expected utility $U(\sigma_n \given \sigma_{1:n-1})$ with a particle filter}
  \label{algo:expected-utility}
  \begin{algorithmic}[1]
    \REQUIRE Measurement operators $\sigma_1, \dots, \sigma_n$, input $\bx$, models $p_\theta(\by \given \bx), \dots, p_\theta(\by \given \bx, \tau_{1:n}, \sigma_{1:n})$
    \ENSURE Expected utility $U(\sigma_n \given \sigma_{1:n-1})$
    \STATE Let $y_{\sigma_n}$ be label(s) measured by operator $\sigma_n$.
    \STATE Initialize $u \gets (0, \dots, 0)$.
    \FORALL{particles $t \in [T]$}
      \FORALL{$i \in [n-1]$}
      \STATE Sample $\tau_i\oft{t} \sim \p(\by \given \bx, \tau_{1:i-1}\oft{t}, \sigma_{1:i})$
      \ENDFOR
      \STATE Set $\pi(\tau_n) \gets \p(\tau_n \given \bx, \tau_{1:n-1}\oft{t}, \sigma_{1:n})$.
      \STATE Initialize $u\oft{t} \gets (0, \dots, 0)$.
      \FORALL{$\tau_n \in [L]$}
      \STATE Compute $\byt = \argmax_{\by} p_\theta(\by \given \bx, \tau_{1:n}, \sigma_{1:n})$ using MAP inference.
      \STATE Compute $p(y_j) = p_\theta(\by_j \given \bx, \tau_{1:n}, \sigma_{1:n})$ for $j \in [n]$ using marginal inference.
      \STATE Update $u\oft{t}[\tau_n] \gets \E_{p(y_j)}[\ell(y_j, \yt_j)]$.
      \ENDFOR
      \STATE Update the expected utility: $u \gets u + \frac{1}{T} \frac{\sum_{i=1}^L u[i] \pi(i)}{\sum_{i=1}^L \pi(i)}$.
    \ENDFOR
    \STATE Return the expected utility: $u$.
  \end{algorithmic}
\end{algorithm}

For each measurement, we compute the operator maximizing the expected utility $\sigma_n^* = \argmax U(\sigma_n \given \sigma_{1:n-1})$ until we reach a $\sigma_n^* = \sigma_0$.
\todo{(arun): BAD NOTATION! The subscripts refer to members of $\Sigma$, but also the sequence of measurements.}

\subsection{Modeling time}
\label{sec:time}

When asking for multiple requests, we must decide between sending a request for information right now versus when we receive the measurement.
In the latter case, we have more information and can make a better informed decision.
Provided unlimited time, the latter is always optimal, but realistically, we have a finite time window in which to make decisions.

We model this.

\subsection{Optimization}

On receiving a label, we would like to optimize to train our loss.
Because we have only partial labeling, the problem is in fact unsupervised.
We use the stepwise online EM algorithm \cite{liang09online}.

\section{Modeling Crowd Workers}

\todo{(arun): rewrite}

Our learner will be allowed to query humans for additional certainty about individual random variables (nodes) $X_i$.
 Humans will exist in a pool $\sP$.
 An individual human $o_i \in \sP$ (for ``oracle'', using the loosest possible definition of oracle) has a model for expected behavior.
Our learner will be allowed to query humans for additional certainty about individual random variables (nodes) $X_i$.
 Humans will exist in a pool $\sP$.
 An individual human $o_i \in \sP$ (for ``oracle'', using the loosest possible definition of oracle) has a model for expected behavior. Specifically, we model the $o_i$ expected delay to respond to a question, and the error function (i.e. response given the true state of the world $h_{\text{true}}$).

For this work we use a simplified model of error, shared uniformly across crowd workers.
 Previous work \cite{yan2011active} \cite{donmez2008proactive} \cite{golovin2010near} has shown that in an offline setting treating oracles uniformly leads to a loss, but in practice our pool is churning so quickly that we don't have time to learn accurate distinctions between workers.
 We leave a solution to this problem to future work.

When asked about variable $X_j$, human error is modeled as correct (returns $h_{\text{true}}(X_j)$) with probability $1-\epsilon$, and chosen uniformly at random from $D_j$ with probability $\epsilon$.
 We use the notation $Q(o_i, X_j)$ to denote the response from asking human $i$ about random variable $j$.

\begin{equation}
    Q(o_i, X_j) \sim
    \begin{cases}
       \text{uniformly drawn from } D_j = \{1 \ldots K_j\}, & \text{with probability}\ \epsilon \\
      h_{\text{true}}(X_j), & \text{otherwise}
    \end{cases}
 \end{equation}
 
This answer arrives after a delay.
 We model the amount of time the worker takes to answer with a gaussian, parameterized by $\mu$ and $\sigma$.
 We use the notation $\sD(o_i, X_j)$ to denote the delay in response when asking human $i$ about random variable $j$.

\[\sD(o_i, X_j) \sim \sN(\mu, \sigma^2)\]

This is an imperfect model, since it assigns some mass to a negative response time, which is impossible, but given a large $\mu$ and relatively small $\sigma$ the mass assigned to $\sD(o_i, X_j) < 0$ is negligible, and it otherwise accurately reflects human response times.

\subsection{The Loss Function}

Our goal is to choose our querying strategy to minimize an arbitrary loss function:
\[\sL(h_{\text{true}}, h, m, t)\]
$\sL$ takes as parameters the true state of the world $h_{\text{true}}$, our guess $h$, and the money $m$ and time $t$ spent arriving at our guess.
 By tweaking $\sL$ practitioners applying our framework can move to arbitrary points on the cost-delay-accuracy tradeoff surface.
 We deliberately make no assumptions about the form of $\sL$ while developing theory, and demonstrate several choices in our experiments.

