\begin{epigraph}
``Poor is the pupil who does not surpass his master.''\\
-- Leonardo da Vinci
\end{epigraph}

\section{Introduction}
\label{sec:intro}

% Setting: want to deploy system without training examples
% Existing solutions: pure ML, pure crowd
There are two roads to an accurate AI system today:
(i) gather a huge amount of labeled training data \citep{deng2009imagenet} and do supervised learning \citep{krizhevsky2012imagenet};
or (ii) use crowdsourcing to directly perform the task \citep{bernstein2010soylent,kokkalis2013emailvalet}.
However, both solutions require non-trivial amounts of time and money.
In many situations, one wishes to build a new system --- e.g., to do Twitter information extraction
\citep{li2012twiner} to aid in disaster relief efforts or monitor public
opinion --- but one simply lacks the resources to follow either the pure ML or pure crowdsourcing road.

% Our proposal: on the job training setting, related work
In this paper, we propose a framework called \emph{on-the-job learning} (formalizing and extending ideas first implemented in \citep{lasecki2013real}),
in which we produce high quality results from the start without requiring a trained model.
When a new input arrives,
the system can choose to asynchronously query the crowd on \emph{parts} of the input it is
uncertain about (e.g. query about the label of a single token in a sentence). After collecting enough evidence the system makes a prediction.
The goal is to maintain high accuracy by initially using the crowd as a crutch,
but gradually becoming more self-sufficient as the model improves.
Online learning \citep{cesabianchi06prediction} and
online active learning \citep{helmbold1997some,sculley2007online,chu2011unbiased}
are different in that
they do not actively seek new information \emph{prior} to making a prediction,
and cannot maintain high accuracy independent of the number of data instances seen so far.
Active classification \citep{gao2011active}, like us,
strategically seeks information (by querying a subset of labels) prior to prediction,
%also queries a subset of the labels strategically,
but it is based on a static policy, 
whereas we improve the model during test time based on observed data.

% Bayesian decision theory: technical solution
To determine which queries to make,
we model on-the-job learning as a stochastic game based on a CRF prediction model.
We use Bayesian decision theory to tradeoff latency, cost, and accuracy in a principled manner.
Our framework naturally gives rise to intuitive strategies:
To achieve high accuracy, we should ask for redundant labels
to offset the noisy responses.  To achieve low latency, we should issue queries
in parallel, whereas if latency is unimportant, we should issue queries
sequentially in order to be more adaptive.
Computing the optimal policy is intractable,
so we develop an approximation
based on Monte Carlo tree search \citep{kocsis2006bandit} and
progressive widening to reason about continuous time \citep{coulom2007computing}.

% Experiments.
We implemented and evaluated our system on three different tasks: named-entity
recognition, sentiment classification, and image classification.
On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8\% F1 improvement over having a single human label the whole set, and a 28\% F1 improvement over online learning.
An open-source implementation of our system, dubbed LENSE for ``Learning from Expensive Noisy Slow Experts'' is available at
\href{http://www.github.com/keenon/lense}{http://www.github.com/keenon/lense}.
%\href{http://anonymo.us}{http://anonymo.us}.


% VKeenon
% % Setting: want to deploy system without training examples
% % Existing solutions: pure ML, pure crowd
% Modern machine learning (ML) and crowdsourcing techniques can both solve the problem of
% providing classifications for questions like ``what's shown in the picture?'' at scale \citep{deng2009imagenet,krizhevsky2012imagenet}.
% % Both methods have been used to power consumer facing products that require intelligent classifications \emph{in real time} \cite{bigham2010vizwiz,guha2015user}.
% From the perspective of an application developer who needs a solution to this problem, the two approaches have very different strengths.
% ML is able to produce classifications with zero marginal cost and millisecond delay, but production quality systems are almost always built using supervised training (e.g. \citep{krizhevsky2012imagenet}) which require massive and expensive (e.g. \citep{deng2009imagenet}) up-front data labelling efforts.
% ML accuracy is limited by the size of the labeled data available, and attempts to increase system accuracy by labeling additional data suffer from diminishing returns.
% Real-time crowdsourcing techniques, by contrast, can use redundant voting to achieve arbitrary accuracies, and require no up-front data labelling, but marginal costs are large compared to ML, and classification delays can exceed tens of seconds \citep{bernstein2010soylent,kokkalis2013emailvalet}.
% 
% % Our proposal: on the job training setting, related work
% In this paper, we propose a new framework called \emph{on-the-job learning} (formalizing and generalizing ideas first implemented in \citep{lasecki2013real}), in which we smoothly transition a \emph{deployed} system from pure crowdsourcing toward unassisted ML.
% Our goal is to deliver the best of both worlds for the application developer: a high-accuracy, low latency system with no expensive up-front labelling and a decreasing marginal cost of classification.
% The on-the-job learning setting makes this possible by letting a deployed ML system query crowd workers \emph{prior to} making each classification.
% This presents the challenge of how to balance the cost and delay of an additional query against the benefit of more confidence when making classifications.
% We formalize this challenge and present heuristic solutions.
% Querying humans (or choosing not to) when making classifications allows on-the-job learning to avoid the up-front cost of data labelling and achieve the arbitrary accuracy of crowdsourcing, all while slowly reducing the marginal cost of decisions toward zero as the ML system's model learns from crowd responses and reduces its reliance on human assistance.
% 
% % To related work?
% %While active learning \citep{settles2010active} and active classification \citep{gao2011active} also query a subset of the labels strategically,
% %they deploy a static predictor in the end, whereas we improve the model during test time.
% %While online learning \citep{cesabianchi06prediction} and
% %online active learning \citep{helmbold1997some,sculley2007online,chu2011unbiased}
% %improve the model during test time,
% %they do not actively seek new information \emph{prior} to making a prediction,
% %and cannot maintain high accuracy independent of the number of data instances seen so far.
% 
% % Bayesian decision theory: technical solution
% To determine which queries to make, and when, we model on-the-job learning as a stochastic game based on a CRF prediction model.
% Unlabeled CRFs arrive needing accurate labels as quickly as possible.
% The system is allowed to asynchronously query for (noisy) human opinions about each of the variables in the model separately.
% It can query as many times as it likes, but must return a set of labels as quickly and cheaply as possible.
% We use Bayesian decision theory to tradeoff latency, cost, and accuracy in a principled manner.
% If we desire high accuracy, we must incur the cost of having more redundant labels to offset the noisy human responses. 
% If we desire low latency, then we should issue queries in parallel, whereas if latency is not a factor, we should issue queries sequentially in order to be more adaptive.
% Computing an optimal policy is intractable, so we employ an approximation based on a new combination of TD learning \citep{sutton1988learning} and Monte Carlo tree search \citep{kocsis2006bandit}.
% 
% % Experiments.
% We implemented and evaluated our system on three different tasks: named-entity recognition, sentiment classification, and image classification.
% In each experiment we achieve a 3-7 fold reduction in cost over the pure crowdsourcing approach that achieves comparable accuracies and latencies.
% We substantially beat the performance of single vote crowd worker opinions on each task, while costing less or comparable amounts.
% Compared to pure ML approaches that see labeled training data but are not allowed to query workers at test time, we (unsurprisingly, as we have humans at test time) obtain much higher accuracy, though at a small additional average cost per example.
% An open-source implementation of our system, dubbed LENSE for ``Learning from Expensive Noisy Slow Experts'' is available at
% \href{http://www.github.com/lense-project/lense-base}{http://www.github.com/lense-project/lense-base}.

%% Story hook: why do you want test time classifications?
%Consider the following unfortunate scenario:
%a natural calamity has just befallen your hometown.
%Charitable organizations have been distributing food, water and clothing, but do not know how to reach out to those in need.
%On social media, people are sharing news of where they have found resources and others are requesting for help.
%However, with so many messages, it is hard to identify which are relevant.
%You would like to identify critical information in these messages to help with the organization of relief efforts, but have no data.
%Can you deploy a system anyway?
%
%% Proposition our actual system.
%There is a long tail of performance critical problems with little to no training data at all\footnote{In fact, the people with big data is a measure zero set.}.
%The existing paradigm advocates separately gathering data (e.g.\ via crowdsourcing) and then training a model.
%However, with limited data, it is hard if not impossible to train an accurate classifier, while larger datasets can take thousands of person hours to construct\findcite{AMR, ImageNet}. 
%This paper posits that there is another way to distribute the same human effort to achieve better results faster: {\em use the humans at test time\/} to train a machine learning system {\em on the job}.
%We show that it is possible to either dramatically reduce or eliminate the creation of training sets all together while still producing classifications with accuracies of $> 90\%$ F1 within a few seconds for significantly less than it would cost to get noisy offline labels for the same data.
%
%% High level
%Our system takes in a prediction model and a utility function that trades off three key parameters: latency, cost and classification accuracy.
%On a stream of test examples, it uses its model to identify parts of the output that it is not confident about and queries the crowd as necessary (\figureref{crf}).
%The model can make several asynchronous queries, incorporating responses whenever to reduce latency.
%Finally, it uses responses to update its model so that, over time, the model learns and requires less human assistance.
%
%% Technical level
%We use a Bayesian decision-theoretic framework to decide if, when and what queries to ask the crowd by modeling the utility of possible outcomes. 
%Inference is intractable, so we use Monte Carlo tree search \ac{stress this is novel}.
%If the given utility function places a requirement on low latency, our model makes many parallel requests, while if we value cost more, the model makes sequential requests so that in can incorporate information maximally.
%
%% Compare with existing work
%\ac{compare with existing work.}
%
%% Experiments.
%We have implemented and evaluated our system on three different tasks: named entity recognition, image classification and sentiment classification on tweets, an get on average \todo{95\%} accuracy, \todo{60\%} reduction in cost at a latency of \todo{3} seconds per classification.
%An open-source implementation of our system is available at \href{http://anonymo.us}{http://anonymo.us}.

%An alternate approach is to use cut out the middle man altogether and use the crowd to label all examples in real-time\cite{cheng2015flock}. 
%Literature in the crowdsourcing literature focuses on using .
%Active learning chosing examples from a pool, for training and not test.






%%\begin{epigraph}
%%``It ain't what you don't know that gets you into trouble.\\
%%It's what you know for sure that just ain't so.'' \\
%%-- Mark Twain
%%\end{epigraph}
%
%\pl{well-motivated, but could be more shorter/more direct; need to make the two regimes crystal clear}
%\pl{I think want to stress the fact that in a lot of cases,
%  we have no labeled data (people might be used to thinking of Google and Big Data);
%  actually, I think it'd be more engaging to start out with the scenario:
%  imagine you're a company (non-profit?) and want to do [something with social good] but have no data
%  (but not get into details).
%  What are your options?
%}
%\pl{Do we want to mention the fact that crowds are also noisy and it'd be nice to figure out adaptively
%  how much redundancy to ask for?  Has no one done this?}
%For the practitioner, supervised learning is largely a solved problem and software to efficiently train classifiers in a variety of domains is readily available.
%Yet, adoption remains limited because the deployment of a learning system requires extremely accurate classifiers: mistakes cost business.
%The typical solution to this problem is to use more labeled data, which recent crowdsourcing platforms such as Amazon Mechanical Turk or CrowdFlower have made relatively affordable to obtain\findcite{citefest!}.
%However, this not only presupposes the ready availability of large amounts of unlabeled data, but also advocates a long, expensive data collection process for uncertain improvements in accuracy.
%% This two-stage process fundamentally limits the accuracy we can obtain.
%An alternate approach is to use cut out the middle man altogether and use the crowd to label all examples in real-time\cite{cheng2015flock}. 
%While this allows us to ensure accurate responses, it becomes prohibitively expensive to scale to more data. 
%
%KEY ELEMENTS: actively query on the test set. how do we actively query? Use magical TD-tree. NEW in the active learning literature.
%
%
%
%Our approach interpolates between these two regimes: we query the crowd in real-time when our model is unsure and learn on the crowd responses to improve on future input.
%This results in a system that provides users high quality responses in real-time while starting {\em with no training data}.
%In this paper, we explicitly address three main challenges that arise in doing so: keeping costs low, guaranteeing low latency and maximizing accuracy.
%\pl{say three resources that we need to tradeoff, explain briefly}
%
%Let us explore an example use-case for such a framework:
%\begin{example*}[Extracting fields from online classifieds]
%  \todo{arun: make sexy!}
%  You wish to create an online classifieds for apartment listings. 
%  Your USP is to automatically identify salient features like the number of bedrooms, housing features, the date of an open house, etc. to allow users to search for listings that match their requirements.
%  A successful service will receive thousands of such postings every day, making manual classification impractical.
%
%  Something about starting with zero training data.
%
%  \todo{arun: maybe the rest of the description of the system is in relation to this example}
%  Using the system described in this paper, you, the owner, need to specify a model to use - in this case, a linear chain conditional random field is relevant.
%  The next challenge is getting data. 
%  On each example we can get crowd workers to identify these features, an easy task for people.
%  \pl{you don't mean features as in ML?}
%  There is a variety of tasks that you can ask people - identifying dates might be easier than not. 
%  Could ask people to identify what a particular word, say pool, is.
%  Alternatively, you could ask people to identify the nearest landmark.
%
%  This data incorporated into the system to provide a more accurate labeling in the future.
%\end{example*}
%
%% Cost and Low-latency
%We focus on structured classification problems using conditional exponential families, a general model class that has been used to \todo{citefest}. In such a model, we are given an input, $\bx$, and must predict a number of labels $\by = y_1, \ldots y_n$.
%
%We treat crowd workers as a resource that can provide noisy measurements of some subset of the labels.
%Under time and budget constraints, we must optimize over which labels to query when. 
%Often, several queries are required on a particular label because of annotator errors by the crowdworker, while at other times it is better to distribute queries across labels.
%Similarly, observing a label from one worker lets us decide better which label to query next, but we might not have to time to wait for the response.
%
%We propose an active classification approach using Bayesian decision theory that is able to make these complex behavioral decisions.
%This quickly leads to an intractable optimization problem that grows exponentially in complexity with the number of label queries we might ask on a single example.
%We propose a novel approximation based on Monte Carlo tree search that retains the behavior of the original Bayesian approach while being computationally tractable.
%
%\todo{(arun): We should make the distinction with active learning much stronger since that's what everyone thinks we're doing. I've moved the related work section to the latter parts because I feel like it's easier to compare our work with existing work given our model.} 
%
%Finally, in practice, labels from crowd workers are often inaccurate.
%We use the measurements framework of Liang et.\ al\cite{liang09measurements} to incorporate noisy responses from crowdworkers in our model.
%Having an accurate estimate of the error rates for crowd workers is essential to accurately predicting how many queries are required.
%Prior work\findcite{unsupervised crowd labeling} uses inter-annotator agreement to predict per-user error rates in an unsupervised manner. 
%The online nature of our task limits the number of responses we have on the same label.
%Instead, we learn the error rates in an unsupervised fashion using online EM, which also allows us to incorporate unlabeled data.
%\pl{measurements should come earlier in the framework - what kind of measurements can we get?}
%
%% Experiments
%We evaluate our system on four different tasks: named entity recognition, information extraction from user generated content, image classification and sentiment classification on tweets.
%We show that by querying crowdworkers at classification time, we can significantly outperform a system trained on fully labeled data, for a fraction of the cost of the baseline of asking crowdworkers for labels for each example, as well as the system trained on fully labeled past data.
%On X of the Y tasks, we produce a classifier comparable with the state of the art while obtaining a much smaller subset of the training labels noisily from the crowd. 
%In fact, we are comparable with state of the art-ish with a handful of the training labels.
%An open-source implementation of our system will be made available.
%
%% Recent work has shown that it is possible to use real-time on-demand workers to power everything from AI-complete email clients~\cite{kokkalis2013emailvalet} to real-time activity surveillance and classification~\cite{lasecki2013real}.
%% These purely crowd-based solutions are prohibitively expensive at scale.
%% Powering the crowd-based email client \textit{EmailValet}~\cite{kokkalis2013emailvalet} for a single end user for a year costs over \$400.
%% 
%% These systems typically work by ``pooling'' on-demand workers from high latency job-posting platforms like Amazon Mechanical Turk or CrowdFlower on a website designed by the system architect~\cite{lasecki2011real}.
%%  The ``pooling'' process can take several minutes, but once in place the workers can be queried at very low latencies by pushing requests to their web-browsers.
%%  This pool of workers can demonstrate high rates of turnover, and unreliability amongst individual annotators.
%% 
%% Existing systems query this pool directly, allowing for annotator noise by incorporating consensus building systems like voting and chat.
%% 
%% 
%% Active classification~\cite{greiner2002learning}, a close sibling of active learning, is a setting in which a classifier is allowed to query for more information, at some cost, before turning in classifications.
%%  This active classifier is intended to reduce its need for costly, slow human labels over time by learning from past observations.
%%  We propose to adapt the active classification framework to the pooled-worker setting to query this pool more cheaply, accurately, and quickly, without sacrificing the advantages live of crowd-powered interfaces.
%% 
%% Previous work in online active learning (which is closely related to what we're proposing) has focused on multi-class classification (\cite{chu2011unbiased},~\cite{agarwal2013selective},~\cite{cheng2013feedback},~\cite{vzliobaite2011active},~\cite{helmbold1997some}).
%%  Multi-class classification is an insufficiently rich primitive to handle many of the tasks that crowd-workers are enabling in existing systems, like information extraction or object detection.
%%  Instead, we will build our platform around arbitrary log-linear markov network classification, where we assume it is possible to query workers for opinions on individual nodes.
%%  Thus each ``active classification'' in our proposed setting is instantiate with a markov network and involves using model priors trained on previously seen data to choose to query the worker-pool for opinions about nodes, and then returning a classification informed by those opinions.
%% 
%% 
%% This setting poses several distinct challenges that have not been sufficiently addressed in previous literature.
%%  We need to be sensitive to time delays, returning results at least as quickly as the pure-crowd baseline we intend to improve upon.
%%  We also need to be sensitive to inaccurate oracles.
%%  These two criteria, in the pooled-worker setting, means that we need an active classifier who is able to hide latencies of redundant queries by launching them \textit{asynchronously}.
%%  This leads to the two challenges we will address in this paper, which can both be clustered under \textit{optimal asynchronous behavior}: we need to be able to handle the decision to ask for another query or turn in existing results in the presence of ``in-flight requests,'' which can fail due to worker turnover, where our loss term is sensitive to time delay.
%%  We draw inspiration from work in Bayesian Active Learning to tackle these problems.
%
%\pl{Here's how I think about this (each item is like a paragraph):
%  \begin{itemize}
%    \item Setting: want to deploy a system (give example), but start with zero examples.
%      Traditional paradigm is gather data (e.g., via crowdsourcing) and then train model.
%      Provocative thought: can we deploy it immediately?
%      Related work: crowdsourcing work is concerned about gathering a dataset or deploy, and only uses ML to clean up.
%      Active learning: about choosing examples from a pool, only for training not online/test.
%    \item High-level solution and its properties (describe the setup):
%      our system takes in latency tolerance, a desired error rate, and a budget (actually two of these).
%      Takes in stream of \emph{test} examples;
%      it queries crowd as necessary about parts of the input to gather more confident
%      and answers.
%      It makes requests asynchronously, and requests are coming back whenever.
%      As it learns, query the crowd less and less (show plot of this later in paper!)
%    \item Technical solution:
%      we use Bayesian decision-theoretic framework to model possible outcomes.
%      Inference is intractable, use Monte Carlo Tree Search (new!)
%      Show that different behaviors emerge from the principle.
%      For example, if low latency requirement, then make many asynchronous requests;
%      otherwise, delay to get more information.
%    \item Experiments: run on face recognition, NER, and get 95\% accuracy
%      with XXX reduction in cost at a latency of XXX seconds.
%  \end{itemize}
%}
%
