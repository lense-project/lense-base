
\section{Introduction}

\begin{epigraph}
``It ain't what you don't know that gets you into trouble.\\
It's what you know for sure that just ain't so.'' \\
-- Mark Twain
\end{epigraph}

For the practitioner, supervised learning is largely a solved problem and software to efficiently train classifiers in a variety of domains is readily available.
Yet, adoption remains limited because the deployment of a learning system requires extremely accurate classifiers: mistakes cost business.
The typical solution to this problem is to use more labeled data, which recent crowdsourcing platforms such as Amazon Mechanical Turk or CrowdFlower have made relatively affordable to obtain\findcite{citefest!}.
However, this not only presupposes the ready availability of large amounts of unlabeled data, but also advocates a long, expensive data collection process for uncertain improvements in accuracy.
% This two-stage process fundamentally limits the accuracy we can obtain.
An alternate approach is to use cut out the middle man altogether and use the crowd to label all examples in real-time\cite{cheng2015flock}. 
While this allows us to ensure accurate responses, it becomes prohibitively expensive to scale to more data. 

Our approach interpolates between these two regimes: we query the crowd in real-time when our model is unsure and learn on the crowd responses to improve on future input.
This results in a system that provides users high quality responses in real-time while starting {\em with no training data}.
In this paper, we explicitly address three main challenges that arise in doing so: keeping costs low, guaranteeing low latency and maximizing accuracy.

% Cost and Low-latency
We focus on structured classification problems using conditional exponential families, a general model class that has been used to \todo{citefest}. In such a model, we are given an input, $\bx$, and must predict a number of labels $\by = y_1, \ldots y_n$.

We treat crowd workers as a resource that can provide noisy measurements of some subset of the labels.
Under time and budget constraints, we must optimize over which labels to query when. 
Often, several queries are required on a particular label because of annotator errors by the crowdworker, while at other times it is better to distribute queries across labels.
Similarly, observing a label from one worker lets us decide better which label to query next, but we might not have to time to wait for the response.

We propose an active classification approach using Bayesian decision theory that is able to make these complex behavioral decisions.
This quickly leads to an intractable optimization problem that grows exponentially in complexity with the number of label queries we might ask on a single example.
We propose a novel approximation based on Monte Carlo tree search that retains the behavior of the original Bayesian approach while being computationally tractable.

\todo{(arun): We should make the distinction with active learning much stronger since that's what everyone thinks we're doing. I've moved the related work section to the latter parts because I feel like it's easier to compare our work with existing work given our model.} 

Finally, in practice, labels from crowd workers are often inaccurate.
We use the measurements framework of Liang et.\ al\cite{liang09measurements} to incorporate noisy responses from crowdworkers in our model.
Having an accurate estimate of the error rates for crowd workers is essential to accurately predicting how many queries are required.
Prior work\findcite{unsupervised crowd labeling} uses inter-annotator agreement to predict per-user error rates in an unsupervised manner. 
The online nature of our task limits the number of responses we have on the same label.
Instead, we learn the error rates in an unsupervised fashion using online EM, which also allows us to incorporate unlabeled data.

% Experiments
We evaluate our system on four different tasks: named entity recognition, information extraction from user generated content, image classification and sentiment classification on tweets.
We show that by querying crowdworkers at classification time, we can significantly outperform a system trained on fully labeled data, for a fraction of the cost of the baseline of asking crowdworkers for labels for each example, as well as the system trained on fully labeled past data.
On X of the Y tasks, we produce a classifier comparable with the state of the art while obtaining a much smaller subset of the training labels noisily from the crowd. 
In fact, we are comparable with state of the art-ish with a handful of the training labels.
An open-source implementation of our system will be made available.

% Recent work has shown that it is possible to use real-time on-demand workers to power everything from AI-complete email clients~\cite{kokkalis2013emailvalet} to real-time activity surveillance and classification~\cite{lasecki2013real}.
% These purely crowd-based solutions are prohibitively expensive at scale.
% Powering the crowd-based email client \textit{EmailValet}~\cite{kokkalis2013emailvalet} for a single end user for a year costs over \$400.
% 
% These systems typically work by ``pooling'' on-demand workers from high latency job-posting platforms like Amazon Mechanical Turk or CrowdFlower on a website designed by the system architect~\cite{lasecki2011real}.
%  The ``pooling'' process can take several minutes, but once in place the workers can be queried at very low latencies by pushing requests to their web-browsers.
%  This pool of workers can demonstrate high rates of turnover, and unreliability amongst individual annotators.
% 
% Existing systems query this pool directly, allowing for annotator noise by incorporating consensus building systems like voting and chat.
% 
% 
% Active classification~\cite{greiner2002learning}, a close sibling of active learning, is a setting in which a classifier is allowed to query for more information, at some cost, before turning in classifications.
%  This active classifier is intended to reduce its need for costly, slow human labels over time by learning from past observations.
%  We propose to adapt the active classification framework to the pooled-worker setting to query this pool more cheaply, accurately, and quickly, without sacrificing the advantages live of crowd-powered interfaces.
% 
% Previous work in online active learning (which is closely related to what we're proposing) has focused on multi-class classification (\cite{chu2011unbiased},~\cite{agarwal2013selective},~\cite{cheng2013feedback},~\cite{vzliobaite2011active},~\cite{helmbold1997some}).
%  Multi-class classification is an insufficiently rich primitive to handle many of the tasks that crowd-workers are enabling in existing systems, like information extraction or object detection.
%  Instead, we will build our platform around arbitrary log-linear markov network classification, where we assume it is possible to query workers for opinions on individual nodes.
%  Thus each ``active classification'' in our proposed setting is instantiate with a markov network and involves using model priors trained on previously seen data to choose to query the worker-pool for opinions about nodes, and then returning a classification informed by those opinions.
% 
% 
% This setting poses several distinct challenges that have not been sufficiently addressed in previous literature.
%  We need to be sensitive to time delays, returning results at least as quickly as the pure-crowd baseline we intend to improve upon.
%  We also need to be sensitive to inaccurate oracles.
%  These two criteria, in the pooled-worker setting, means that we need an active classifier who is able to hide latencies of redundant queries by launching them \textit{asynchronously}.
%  This leads to the two challenges we will address in this paper, which can both be clustered under \textit{optimal asynchronous behavior}: we need to be able to handle the decision to ask for another query or turn in existing results in the presence of ``in-flight requests,'' which can fail due to worker turnover, where our loss term is sensitive to time delay.
%  We draw inspiration from work in Bayesian Active Learning to tackle these problems.



