\section{Model}
\label{sec:model}

% Define game top-down
The central technical challenge in on-the-job training
is to determine which queries $q_1, q_2, \dots$ to issue on an input $\bx$
any example and when.  For this, we appeal to Bayesian decision theory.
We define a stochastic game with two players, the system and the crowd.
We start by considering the simplified game tree in \figureref{piano-roll}(right).
At the root, the system chooses an query $q \in \{1, \dots, n\}$ to label a position,
and then the crowd responds with $r \in \{ \scper{}, \scloc{}, \scnone{} \}$.
Each leaf of the game tree represent a \emph{utility} $U(q, r)$ of that outcome.
The \emph{value} of the game is the maximum expected utility:
\begin{align}
  V^* = \max_q \E_{p(r \mid q)}[U(q, r)],
\end{align}
and the \emph{optimal policy} is to choose the action $q$ that attains $V^*$.
Let's look at the example more intuitively:
querying one of the end positions ($q = 1$ or $q = 3$),
is less informative than choosing the middle position ($q = 2$),
assuming the model propagates information between adjacent positions.
Indeed, the expected utilities with a uniform distribution over $r$
are $0.7$, $0.8$, and $0.7$, respectively, and so $q = 2$ is the optimal action.
Now, it remains to specify the transitions $p(r \mid q)$ and the utilities $U(q, r)$.

\paragraph{Prediction model.}
We consider the family of conditional exponential models, a popular class of models that include logistic regression and conditional random fields.
Let $\bx$ be a given input, then the labels $\by = y_1, \ldots, y_n \in \{1, \dots, L\}$ are generated by the following conditional distribution:
\begin{align*}
  \p(\by \given \bx) 
  &= \exp( \theta^\top \phi(\bx, \by) - A(\theta; \bx)),
\end{align*}
where $\theta$ are the model parameters,
$\phi(\bx, \by)$ are arbitrary features of the input and labels and 
$A(\theta; \bx)$ is the conditional log-normalizer.
We assume that the model has low treewidth (e.g.\ $\phi$ factorizes over the labels $\by$) or otherwise admits efficient marginal computation.

For example, the model in \figureref{crf} is a linear-chain conditional random field. The input is the sequence of words in the tweet and the output is a label in the set \scnone, \scres, \scloc{} and \scper. Marginal inference can be efficiently computed using the Viterbi algorithm.

%Conventionally, we are given a training dataset $\sD = \{\bx_i, \by_i\}$ and can learn $\theta$ by optimizing the convex log-likelihood objective $\sL(\theta) = \sum_{t=1}^T \log \p(\by\oft{t} \given \bx\oft{t})$.
%In our setting, however, we do not observe the gold labels $\by$. 
%Instead, we can ask the crowd to provide a ``measurement'' for some subset of the labels.
%Let $\Sigma = \{\sigma_i\}$ be the set of possible measurements we can ask the crowd for and 
%let $\by_\sigma \subseteq \by$ be the subset of labels queried.

\paragraph{Response model.}
Let $q \in \{1, \ldots, n\}$ be a query on for the label $y_q$.
We model the response, $r$, with an exponential measurement model:
\begin{align*}
  p_\beta(r \given x, y, q) 
  &\propto \exp \left( \beta^\top\psi(\bx, y_q, r) \right),
\end{align*}
where $\beta$ and $\psi$ are extra parameters and features for the human error model. 
%The choice of an exponential model allows us to simply include measurements as an additional factor.
\figureref{crf}(c) shows the original graph with additional measurement nodes.
A simple human error model is to return the true label with probability $1-\epsilon$ and a random label otherwise.
In our running example, some classes, e.g.\ $\textsc{none}$, are more easily identified than others: in this setting responses can be modeled to be sampled from a confusion matrix.

Finally, we model delay to be drawn from a Gamma distribution: $\tau \sim \Gamma$\footnote{We assume here for simplicity that the response delays are independent of the input and which label is being queried. The model can easily be generalized to incorporate these settings.}.
\ac{The gamma is missing parameters.}
Note that the total time taken on a prediction, $t$, depends not only on how many requests are made, but also when they are scheduled.
%We study the problem of how to best schedule multiple requests in \sectionref{async}.

Next, we describe how we use our models to predict labels and learn from partial feedback.

\paragraph{Making predictions.}
Given parameters $\theta, \beta$ and responses $r_1, \ldots, r_m$, our model makes predictions using maximum likelihood:
$\byt \given \bx, r_1, \ldots r_m = \argmax_{\by} \p(\by \given \bx, r_1, \ldots r_m)$.

\paragraph{Learning from responses.}
Recall that we do not have gold labels for our data, but only noisy measurements: we do not have supervised examples to learn from. 
As a simple heuristic, we use the output from our model as gold labels and update our parameters $\theta$ periodically.  
We consider the response parameters $\beta$ to be fixed a priori.
The time delay parameters can easily be estimated from the observed response delays.
%In future work, we plan to explore using (online) expectation-maximization to jointly learn parameters for our model and the human error model in an unsupervised fashion.

\paragraph{Computing expected utility.}

\ac{Text}
We cast this problem in the Bayesian decision theoretic framework: our objective is to maximize our expected utility under our current model,
$\p(\by \given \bx, \br)$:
\begin{align*}
  u &= \E_{\by \sim \p(\cdot \given \bx, \br)}[1 - \ell(\by, \byt) + C(\bq, t)].
\end{align*}

