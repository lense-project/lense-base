\section{Related Work}
\label{sec:related}

% PL: need to broaden out more

On-the-job learning draws ideas from many areas:
online learning, active learning, active classification, crowdsourcing, and structured
prediction.
%\textbf{1)} learning on streaming data with optional observations,
%and \textbf{2)} actively querying at prediction time.

%\paragraph{1) Learning on streaming data optionally observed data:}

% Online learning
\textbf{Online learning.}
The fundamental premise of online learning is that algorithms should improve
with time, and there is a rich body of work in this area \citep{cesabianchi06prediction}.
In our setting, algorithms not only improve over time, but maintain high accuracy from the beginning,
whereas regret bounds only achieve this asymptotically.

% Active learning
\textbf{Active learning.}
Active learning (see \citet{settles2010active} for a survey) algorithms
strategically select most informative examples to build a classifier.
Online active learning
\citep{helmbold1997some,sculley2007online,chu2011unbiased}
performs active learning in the online setting.
Several authors have also considered using crowd workers as a noisy oracle
e.g. \citep{donmez2008proactive,golovin2010near}. %,yan2011active,vijayanarasimhan2014large}.
It differs from our setup in that 
it assumes that labels can only be observed {\em after} classification,
which makes it nearly impossible to maintain high accuracy in the beginning.

%On-the-job learning differs from Online Active Classification because it allows
%to querying {\em before} classification and must trade off cost, latency and
%accuracy while doing so.

\textbf{Active classification.}
Active classification~\cite{greiner2002learning,chai2004test,esmeir2007anytime}
asks what are the {\em most informative features} to measure at test time.
Existing active classification algorithms rely on having a fully labeled
dataset which is used to learn a static policy for when certain features should
be queried, which does not change at test time.
On-the-job learning differs from active classification in two respects: true
labels are {\em never} observed, and our system improves itself at test
time by learning a stronger model.
A notable exception is Legion:AR~\cite{lasecki2013real},
which like us operates in on-the-job learning setting
to for real-time activity classification.
However, they do not explore the machine learning foundations associated
with operating in this setting, which is the aim of this paper.
%with HMMs and crowd-workers.
%However, the authors do not address the exciting machine learning problems presented by the on-the-job setting.
%We aim to provide the analysis of on-the-job learning as a broader setting in this paper.

\textbf{Crowdsourcing.}
%Using crowd workers to assist in various tasks is an area of active research
%within the HCI community
A burgenoning subset of the crowdsourcing community overlaps with
machine learning. %\citep{bernstein2010soylent,kokkalis2013emailvalet}.
One example is \textit{Flock}~\cite{cheng2015flock}, which first crowdsources the identification of
features for an image classification task, and then asks the crowd to annotate
these features so it can learn a decision tree.
% PL: no need to bash, completely different
%As a consequence, the system always relies on humans to provide features, and
%cannot hope to reach the limit case where humans are no longer needed, as in
%on-the-job learning.
In another line of work, \textit{TurKontrol}~\cite{dai2010decision} models
individual crowd worker reliability to optimize the number of human votes
needed to achieve confident consensus using a POMDP\@.

\textbf{Structured prediction.}
An important aspect our prediction tasks is that the output is structured,
which leads to a much richer setting for one-the-job learning.
Since tags are correlated, the importance of a coherent framework for optimizing
querying resources is increased.
Making active partial observations on structures
and has been explored in the measurements framework of \citet{liang09measurements}
and in the distant supervision setting \citet{angeli2014combining}.

%This work tries to relax the traditional assumptions in active learning that the oracle is infallible and has no economic cost.
%Some of this work is directly motivated by applications to crowd-sourcing platforms that we investigate.
%WHY DIFFERENT?

% Bayesian priors we use not.
%Finally Bayesian active learning (\cite{golovin2010near},~\cite{tong2000active}) allows us to incorporate a Bayesian prior over our data, and we'll use this as a foundation for our approach to solving the asynchronous behavior problem.

%We also model the reliability of workers though using an unsupervised model, similar to \findcite{crowd em}.
% Finally, recent work has studied how to support real-time behavior with crowd workers~\cite{bernstein2011crowds,lasecki2013real} by hiring workers ``on retainer''.
% We use the same retainer model to maintain a pool of real-time crowd workers with low response times.

% Using crowds to power decision making is not a new idea. Systems in this space that support real-time behavior include \textit{Adrenaline}~\cite{bernstein2011crowds} and \textit{Legion AR}~\cite{lasecki2013real}, which both use a system where crowds are recruited ``on-retainer'' in order to be available at a moments notice.
% We use the same retainer model to maintain a pool of real-time workers with low response times.
%Using artificial-intelligence-crowd hybrids for time-insensitive workflows has also been previously explored.
%It makes no effort to train a model to augment or take over from the workers, so costs remain constant over time.
%Empirical studies have shown that this is an effective method for managing complex workflows \cite{peng2011artificial}.
%Our work follows \cite{peng2010decision} in that we apply Decision Theory to the problem of when to query the crowd, but we train a model to take over for the workers over time, and we handle the additional real time response constraint.

%\ac{I omitted partial monitoring games because it seems to be covered in the algo section.}
%\paragraph{Partial monitoring games.}
%\noteb{This bullet point is actually to motivate that our problem is theoretically feasible.}
%Our evaluation metric is unique in the active learning space in that consider a loss we do not observe because we never receive true labels.
%By treating the measurements as partial feedback, our work can be theoretically modeled as a partial monitoring game\cite{cesabianchi06regret} and, in particular, an instance of the label-efficient learning problem\cite{cesabianchi05minimizing}.
%Cesa-Bianchi et al.~\cite{cesabianchi06regret} show that in the online setting, the regret of a partial monitoring game is lower bounded by $O(T^{2/3})$, where $T$ is the number of examples seen. They also provide an algorithm that meets this bound: use the current model to pick the best label and query for complete labels at random with a small probability to update your model.
%These guarantees provide theoretical foundation for our work.

% We already discussed expectimin trees in the relevant section.
%\paragraph{Expectimin trees and game playing}
%Monte Carlo based search methods~\cite{browne2012monte} are a common solution for finding a policy for an expectimin tree.
%Our usage is slightly different in that we do not need to explore infinite depth.
%Monte Carlo tree search can be extended to continuous state spaces using progressive widening.
%However, because so much information is shared across time, we propose a slight modification of the Dyna-2\cite{silver2008sample} algorithm which proposes features for a linear state value function.
%we do not use separate local features because our
%- Using state has been explored in Dyna-2 - we do not use a separate set of local features because the state space isn't sufficiently different.
