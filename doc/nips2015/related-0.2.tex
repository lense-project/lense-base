\section{Related Work}
\label{sec:related}

The on-the-job learning setting brings together two orthogonal ideas: \textbf{1)} learning classifiers in the presence of streaming optionally observed data, and \textbf{2)} querying oracles {\em during classification} for additional certainty.

\paragraph{1) Learning classifiers in the presence of streaming optionally observed data:}

Online Active learning\footnote{We refer the readers to~\cite{settles2010active} for a survey of active learning methods and variants.} is a popular approach to minimize the labeled data required to train a classifier. It assumes data that can be optionally observed at some cost, {\em after} classification.
Several authors have considered using crowd workers as a noisy oracle for Online Active Learning~\cite{donmez2008proactive,golovin2010near,yan2011active,vijayanarasimhan2014large}.
On-the-job learning differs from Online Active Classification because it allows to querying {\em before} classification and must trade off cost, latency and accuracy while doing so.

Making partial observations on structures is an important part of on-the-job learning, and has been explored in the past:
Angeli et al.~\cite{angeli2014combining} identify instances to label within a cluster of examples in a distantly supervised setting.
Liang et al.~\cite{liang09measurements} introduced the measurement framework and studied the problem of active selection of measurements in the active learning setting.

\paragraph{2) Querying oracles during classification for additional certainty:}

Active classification~\cite{greiner2002learning,chai2004test,esmeir2007anytime} asks what is the {\em most informative feature\/} to measure at test time.
Existing active classification algorithms rely on having a fully labeled dataset which is used to learn a static policy for when certain features should be queried, which does not change at test time.
On-the-job learning differs from Active Classification in two respects: true labels are {\em never} observed, and our system must improve itself at test time by learning a stronger model and recomputing policy.

Using crowd workers to assist labeling tasks is an area of active research within the HCI community.
\textit{Flock}~\cite{cheng2015flock} first crowdsources the identification of features for an image classification task, and then asks the crowd to annotate these features so it can learn a decision tree.
As a consequence, the system always relies on humans to provide features, and cannot hope to reach the limit case where humans are no longer needed, as in on-the-job learning.
In another line of work, \textit{TurKontrol}~\cite{dai2010decision} models individual crowd worker reliability to optimize the number of human votes needed to achieve confident consensus using a POMDP\@.

\paragraph{Combining 1 \& 2:}

\textit{Legion AR}~\cite{lasecki2013real} provided a ground-breaking example of an on-the-job task: Learning to perform real-time activity classification with HMMs and crowd-workers.
However, the authors do not address the exciting machine learning problems presented by the on-the-job setting.
We aim to provide the analysis of on-the-job learning as a broader setting in this paper.

%This work tries to relax the traditional assumptions in active learning that the oracle is infallible and has no economic cost.
%Some of this work is directly motivated by applications to crowd-sourcing platforms that we investigate.
%WHY DIFFERENT?

% Bayesian priors we use not.
%Finally Bayesian active learning (\cite{golovin2010near},~\cite{tong2000active}) allows us to incorporate a Bayesian prior over our data, and we'll use this as a foundation for our approach to solving the asynchronous behavior problem.

%We also model the reliability of workers though using an unsupervised model, similar to \findcite{crowd em}.
% Finally, recent work has studied how to support real-time behavior with crowd workers~\cite{bernstein2011crowds,lasecki2013real} by hiring workers ``on retainer''.
% We use the same retainer model to maintain a pool of real-time crowd workers with low response times.

% Using crowds to power decision making is not a new idea. Systems in this space that support real-time behavior include \textit{Adrenaline}~\cite{bernstein2011crowds} and \textit{Legion AR}~\cite{lasecki2013real}, which both use a system where crowds are recruited ``on-retainer'' in order to be available at a moments notice.
% We use the same retainer model to maintain a pool of real-time workers with low response times.
%Using artificial-intelligence-crowd hybrids for time-insensitive workflows has also been previously explored.
%It makes no effort to train a model to augment or take over from the workers, so costs remain constant over time.
%Empirical studies have shown that this is an effective method for managing complex workflows \cite{peng2011artificial}.
%Our work follows \cite{peng2010decision} in that we apply Decision Theory to the problem of when to query the crowd, but we train a model to take over for the workers over time, and we handle the additional real time response constraint.

%\ac{I omitted partial monitoring games because it seems to be covered in the algo section.}
%\paragraph{Partial monitoring games.}
%\noteb{This bullet point is actually to motivate that our problem is theoretically feasible.}
%Our evaluation metric is unique in the active learning space in that consider a loss we do not observe because we never receive true labels.
%By treating the measurements as partial feedback, our work can be theoretically modeled as a partial monitoring game\cite{cesabianchi06regret} and, in particular, an instance of the label-efficient learning problem\cite{cesabianchi05minimizing}.
%Cesa-Bianchi et al.~\cite{cesabianchi06regret} show that in the online setting, the regret of a partial monitoring game is lower bounded by $O(T^{2/3})$, where $T$ is the number of examples seen. They also provide an algorithm that meets this bound: use the current model to pick the best label and query for complete labels at random with a small probability to update your model.
%These guarantees provide theoretical foundation for our work.

% We already discussed expectimin trees in the relevant section.
%\paragraph{Expectimin trees and game playing}
%Monte Carlo based search methods~\cite{browne2012monte} are a common solution for finding a policy for an expectimin tree.
%Our usage is slightly different in that we do not need to explore infinite depth.
%Monte Carlo tree search can be extended to continuous state spaces using progressive widening.
%However, because so much information is shared across time, we propose a slight modification of the Dyna-2\cite{silver2008sample} algorithm which proposes features for a linear state value function.
%we do not use separate local features because our
%- Using state has been explored in Dyna-2 - we do not use a separate set of local features because the state space isn't sufficiently different.
