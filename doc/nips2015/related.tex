\section{Related Work}
\label{sec:related}

\todo{arun: gut}

This work is a generalization of the active classification setting proposed by \cite{greiner2002learning}, adapted to a real-time, noisy annotator environment.
Work on real time active classification with crowds builds upon several related disciplines.
Using crowds to perform tasks is an idea previously explored in several applied settings which we detail below under the heading ``Collective Intelligence,'' which includes human only and hybrid approaches.
Active classification is also a close theoretical cousin of active learning, with the key difference that rather than query oracles during training, we are interested in querying during test time.

\subsection{Collective Intelligence Systems}

Using crowds to power decision making is not a new idea. Systems in this space that support real-time behavior include \textit{Adrenaline}~\cite{bernstein2011crowds} and \textit{Legion AR}~\cite{lasecki2013real}, which both use a system where crowds are recruited ``on-retainer'' in order to be available at a moments notice.
We use the same retainer model to maintain a pool of real-time workers with low response times.

Using artificial-intelligence-crowd hybrids for time-insensitive workflows has also been previously explored.
\textit{Flock}~\cite{chengflock} attempts to put the entire classifier construction process, including proposing and annotating features, into the crowd. \cite{chengflock} has poor cost behavior in the limit because human labels are required for all input features.
\textit{TurKontrol}~\cite{peng2010decision} models individual crowd worker reliability to optimize the number of human votes needed to achieve confident consensus using a POMDP.
%It makes no effort to train a model to augment or take over from the workers, so costs remain constant over time.
%Empirical studies have shown that this is an effective method for managing complex workflows \cite{peng2011artificial}.
Our work follows \cite{peng2010decision} in that we apply Decision Theory to the problem of when to query the crowd, but we train a model to take over for the workers over time, and we handle the additional real time response constraint.

\subsection{Active Learning}

% In order to properly situate our proposal in prior art on Active Learning there's several key similarities to draw between traditional Active Learning and Active Classification.
% Active Learning is primarily concerned with generating a (possibly changing) ordering over a static pool of unlabeled examples, such that if examples are labeled one at a time and the model is retrained at each step, the maximum accuracy is achieved for minimum number of human labels.
% This is traditionally done over a static pool of unlabeled examples.
% To the extent that we can think of the nodes of our CRF queries as (dependent) examples to be labeled, this work is applicable.
% By contrast Active Classification is interested in producing an accurate label for every example arriving in a stream, with the option to query for human labels at test time.
 

There has been a large body of work investigating the ``streaming'' setting for Active Learning (\cite{chu2011unbiased},~\cite{agarwal2013selective},~\cite{cheng2013feedback},~\cite{vzliobaite2011active},~\cite{helmbold1997some}), where the algorithm visits a data instance once, and can choose to either request a label or discard it.
Our labeling decision in the asynchronous delay-sensitive setting is much more complicated than the decisions made by these algorithms, because we are required to turn in a classification and pay a penalty for an incorrect guess.


Active Learning in the context of PGMs has been previously explored from several angles.
Active Learning for CRF sequence models was explored in the context of batch active learning, where the oracle is able to provide labels for an entire sequence at once~\cite{settles2008analysis}.

It's also been explored in the context of fully Bayesian networks where the oracle\cite{tong2000active} can draw samples conditioned on certain ``controllable'' values.
Note: \cite{tong2000active} introduce the notion of expected posterior risk - something we also use.

The ability to request observations of single nodes has also been explored ~\cite{angeli2014combining} - but not in an online setting - they didn't see how much information they'd get on the whole problem by labeling that one thing.


Active Learning has also been explored for specific structure-prediction tasks,~\cite{roth2006active} and~\cite{culotta2005reducing}, where humans perform top-K selection over model predictions.
The systems fall back by stages to traditional no-assistance annotation if the top-K doesn't contain the any correct information.
% While this approach is effective when possible, it relies on the model to consistently produce the correct answer in a top-K for some very small $K$, so for large output spaces it breaks down.

There's been a line of work on Active Learning in the context of multiple noisy, expensive oracles (\cite{yan2011active},~\cite{donmez2008proactive},~\cite{golovin2010near},~\cite{vijayanarasimhan2014large}).
This work tries to relax the traditional assumptions in Active Learning that the oracle is infallible and has no economic cost.
Some of this work is directly motivated by applications to crowd-sourcing platforms that we investigate.
WHY DIFFERENT?


Finally Bayesian Active Learning (\cite{golovin2010near},~\cite{tong2000active}) allows us to incorporate a Bayesian prior over our data, and we'll use this as a foundation for our approach to solving the asynchronous behavior problem.

\subsection{Relation to partial monitoring games.}

First, let's look at how answers produced by the system are evaluated.
In the conventional supervised setting, a natural metric is the accuracy of our predictions relative to the true labels in the training data.
The system gets to see these true labels and is able to optimize an appropriate loss.

Formally, let $\bx\oft{1}, \dots, \bx\oft{t}, \dots, \bx\oft{T}$ be a sequence of inputs, 
let $(\by\oft{t})$ be their true labels,
and let $(\byt\oft{t})$ be the sequence of predictions made by the system.
We measure the performance of our system using the loss $\ell(\by\oft{t}, \byt\oft{t})$, and our objective is to minimize the loss on the dataset: $\sum_{t=1}^T \ell(\by\oft{t}, \byt\oft{t})$.

When constructing datasets from user interactions, the system does not get to observe $\by\oft{t}$ or $\ell(\by\oft{t}, \byt\oft{t})$.
We are only aware of the true labels if and when we query a human annotator: simply measuring performance incurs a cost for our system.
Let $\sigma\oft{t}$ be the measurements queried by the system, incurring a cost $C(\sigma\oft{t})$.
Our goal is to minimize the objective
\begin{align*}
  \sL &= \sum_{t=1}^T \ell(\by\oft{t}, \byt\oft{t}) + C(\sigma\oft{t}).
\end{align*}
