\begin{abstract}
Real-time crowdsourcing allows us to solve traditional artificial intelligence problems for users (e.g. helping the blind by identifying photos taken from a smart phone), but remains prohibitively expensive in practice.
We consider an ``on-the-job'' setting, where a machine learning system can intelligently query a crowd at test time to add confidence to its model before returning classifications, allowing nearly an order of magnitude reduction in human labor (and cost) to achieve the same accuracies.
As the model improves over time, the reliance on crowdsourcing queries decreases.
We cast our setting as a stochastic game based on Bayesian decision theory,
which allows us to balance latency, cost, and accuracy objectives in a principled way.
Computing the optimal policy is intractable, so we develop an approximation
based on Monte Carlo Tree Search.
We tested our approach across three tasks---named-entity recognition, sentiment classification,
and image classification.
On the NER task we obtained a 6--7 fold reduction in cost compared to state-of-the-art real-time human classification (redundant voting).
We also achieve a 17\% F$_1$ improvement over taking a single human vote as the answer to each query,
and a 28\% F$_1$ improvement over an online learning classifier that cannot query humans.
%Learning with little or no data is an important but understudied problem.
%For most tasks, the current paradigm of first collecting a training dataset and then training a classifier requires a substantial amount of data to be labeled to train a very accurate classifier.
%We propose a  that redistributes the same human effort to achieve
%better results faster: 
%query humans in real time at test time to train a machine learning system {\em
%on the job}.
%%\ac{is it clear what we're doing?}
%%The system only queries humans when it is unsure of its output and learns to 
%Querying humans introduces cost and latency but can improve accuracy over our
%model's prediction.
%We explicitly manage these three key objectives by casting the problem as a
%Bayesian decision theoretic control problem
%and draw on techniques from the game-playing literature.
%The resulting live system provides high quality responses in real-time while
%starting {\em with no training data\/}: we maintain $> 90\%$ F1 consistently on
%a stream of test inputs on the three tasks studied at low cost and latency,
%despite noisy labels from crowd workers.
% outperform baselines?
\end{abstract}

% V1
% Recent work in crowd-powered products has demonstrated the potential of using ``crowd workers'' to power live, low latency systems that accomplish AI complete tasks. 
% These systems suffer from high scaling cost, and the unreliability of workers.
% We propose an \textit{active classification} approach to dramatically reduce the scaling cost and improve the accuracy of such systems.
% In order to also achieve low latency, we investigate \textit{asynchronous} active classification, where multiple requests can be simultaneously ``in-flight.''
% This leads to the twin challenges of how to behave optimally in the presence of asynchronous noisy oracle queries that have not yet returned, and when to return a classification that is ``good enough'' in such a setting.
% We first reduce the problem of optimal asynchronous active classification to a Partial Monitoring game, by making use of Bayesian decision theory.
% We show a bound on achievable regret in this setting, and demonstrate practical heuristics that approach this bound.
% We also show adaptations for traditional Active Learning algorithms to our setting.
% We show empirical demonstration of our proposed algorithms, which demonstrate dramatic improvement over the human-only, machine-only, and baseline human-machine hybrid alternatives in the cost-delay-accuracy tradeoff surface.

% V2?
% Recent work in real-time crowd-powered products has demonstrated the possibility of using human ``crowd workers'' to power live, low latency products that accomplish AI complete tasks. Human-only solutions remain expensive, and do not scale into production. We propose a \textit{active classification} approach to dramatically reduce the scaling cost of such systems, backed by a pool of unreliable crowd-workers. In order to achieve this, we investigate \textit{asynchronous active classification}, where multiple requests can be simultaneously ``in-flight.'' This paper analyzes the \textit{asynchronous requests problem} of how to behave optimally in the presence of asynchronous oracle queries that have not yet returned, and the \textit{optimal stopping problem} of when a classification is ``good enough'' in such a setting. Our solutions to these problems enable a system that shows dramatic improvement over the human-only, machine-only, and baseline human-machine hybrid alternatives in the cost-delay-accuracy tradeoff surface.
% With sufficient labeled data and research effort, supervised learning has been applied in performance critical domains.
% Yet, often logistical/financial constraints mean that data can only be collected and labeled in the presence of a working system, or that insufficient data can be collected to achieve performance requirements.

% V3
% We propose a crowd/machine learning hybrid approach to apply supervised learning in performance critical domains where ``cold start'' barriers exist: we allow our model to query the crowd in real-time {\em at test time}, and learn from the crowd responses online to improve performance and reduce costs on future inputs.
% %Querying humans in real time introduces latency, noise, and cost into the system, which must be minimized.
% Querying humans introduces latency, noise, and cost into the system, which must be minimized.
% We explicitly manage these three key objectives by casting the problem as a Bayesian decision theoretic control problem, and drawing on techniques from game-playing literature for real-time solutions.
% The resulting system provides high quality responses in real-time while starting {\em with no training data\/}: we have achieve $> 90\%$ F1 on the three tasks studied at a low cost, despite noisy crowd worker performance.
% % outperform baselines?
